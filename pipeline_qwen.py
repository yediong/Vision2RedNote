# -*- coding: utf-8 -*-
import os
import json
import glob
import asyncio
import base64
import mimetypes
import time
import logging
from openai import AsyncOpenAI
from tqdm import tqdm
from tenacity import retry, stop_after_attempt, wait_exponential

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("descriptions/pipeline.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("QwenVLPipeline")

class QwenVLPipeline:
    """
    åŸºäºQwen-VLçš„å›¾ç‰‡æè¿°ç”Ÿæˆç³»ç»Ÿï¼Œä¸“ä¸ºç”Ÿæˆå°çº¢ä¹¦é£æ ¼æè¿°è®¾è®¡
    """
    def __init__(
        self,
        api_key: str = "sk-5f28127ea6524835b3304469e54d05f5",
        model_name: str = "qwen-vl-max",  # ä½¿ç”¨æ­£ç¡®çš„Qwen-VLæ¨¡å‹åç§°
        base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",  # ä¿®æ­£ï¼šç§»é™¤æœ«å°¾ç©ºæ ¼
        output_dir: str = "descriptions"
    ):
        """
        åˆå§‹åŒ–Qwen-VLç®¡é“
        
        å‚æ•°:
        api_key: DashScope API Key
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œæ¨èqwen-vl-maxæˆ–qwen-vl-plus
        base_url: APIåŸºç¡€URL
        output_dir: è¾“å‡ºç›®å½•
        """
        self.api_key = api_key
        if not self.api_key:
            raise ValueError("API Keyä¸èƒ½ä¸ºç©ºï¼Œè¯·æä¾›æœ‰æ•ˆçš„DashScope API Key")
        
        # ä¿®æ­£ï¼šç¡®ä¿base_urlæ²¡æœ‰å¤šä½™ç©ºæ ¼
        self.base_url = base_url.strip()
        
        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
        )
        
        self.model_name = model_name
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # å°çº¢ä¹¦é£æ ¼çš„ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = """ä½ æ˜¯ä¸€ä½æ‹¥æœ‰50ä¸‡ç²‰ä¸çš„å°çº¢ä¹¦çˆ†æ¬¾ç¬”è®°åˆ›ä½œè€…ï¼Œæ“…é•¿ç”¨æ¸©æš–æ²»æ„ˆçš„è¯­è¨€æ•æ‰ç”Ÿæ´»ä¸­çš„ç¾å¥½ç¬é—´ã€‚è¯·ä¸ºå›¾ç‰‡åˆ›ä½œä¸€ç¯‡é«˜äº’åŠ¨ç‡çš„å°çº¢ä¹¦ç¬”è®°ï¼Œè¦æ±‚ï¼š

1. æ ‡é¢˜å¿…é¡»æœ‰å¸å¼•åŠ›ï¼Œä½¿ç”¨emojiå’Œæ„Ÿå¹å·ï¼Œå¦‚ã€XXXå¤ªæˆ³äº†ï¼âœ¨ã€‘
2. å¼€å¤´ç”¨"è°æ‡‚å•Šå®¶äººä»¬ï¼ï¼"æˆ–ç±»ä¼¼äº²åˆ‡ç§°å‘¼ï¼Œè¥é€ é—ºèœœèŠå¤©æ°›å›´
3. æ­£æ–‡æè¿°3-5ä¸ªç”»é¢ç»†èŠ‚ï¼Œæ¯ä¸ªç»†èŠ‚å‰ç”¨ç‰¹æ®Šç¬¦å·(â¶/â·/â¸)æ ‡æ³¨
4. ä½¿ç”¨è‡³å°‘4ä¸ªç›¸å…³è¯é¢˜æ ‡ç­¾ï¼Œæ ¼å¼ä¸º#æ ‡ç­¾å
5. ç»“å°¾æä¾›"é…å›¾tips"ï¼Œè¯´æ˜æ‹æ‘„æŠ€å·§
6. æ•´ä½“è¯­æ°”æ´»æ³¼ã€æ¸©æš–ã€æ²»æ„ˆï¼Œå……æ»¡ç”Ÿæ´»æ°”æ¯
7. é¿å…ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œåƒæ™®é€šäººåˆ†äº«æƒŠå–œå‘ç°ä¸€æ ·è‡ªç„¶
8. é€‚å½“ä½¿ç”¨ç½‘ç»œæµè¡Œè¯­ï¼Œä½†ä¸è¿‡åº¦

è¯·ç¡®ä¿æè¿°å‡†ç¡®åæ˜ å›¾ç‰‡å†…å®¹ï¼Œä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„å…ƒç´ ã€‚"""
    
    def load_existing_results(self):
        """åŠ è½½ç°æœ‰çš„ç»“æœæ–‡ä»¶"""
        result_path = f"{self.output_dir}/qwen_vl_descriptions.json"
        if os.path.exists(result_path):
            try:
                with open(result_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½ç°æœ‰ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        # è¿”å›ç©ºç»“æ„
        return {
            "train": [],
            "val": [],
            "metadata": {
                "model": self.model_name,
                "system_prompt": self.system_prompt,
                "total_images": 0,
                "timestamp": 0,
                "api_key_used": self.api_key[:8] + "..." if self.api_key else "N/A"
            }
        }
    
    def get_processed_images(self, results=None):
        """è·å–å·²å¤„ç†çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨"""
        if results is None:
            results = self.load_existing_results()
        
        processed_images = []
        if 'train' in results:
            processed_images.extend([item['image_path'] for item in results['train'] if 'image_path' in item])
        if 'val' in results:
            processed_images.extend([item['image_path'] for item in results['val'] if 'image_path' in item])
        return processed_images
    
    def _get_image_mime_type(self, image_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–æ­£ç¡®çš„MIMEç±»å‹"""
        ext = os.path.splitext(image_path)[1].lower()
        if ext in ['.jpg', '.jpeg']:
            return "image/jpeg"
        elif ext == '.png':
            return "image/png"
        elif ext == '.webp':
            return "image/webp"
        else:
            mime_type, _ = mimetypes.guess_type(image_path)
            if mime_type and mime_type.startswith('image/'):
                return mime_type
            return "image/jpeg"  # é»˜è®¤ä½¿ç”¨JPEG
    
    @retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=2, min=2, max=10))
    async def generate_description(self, image_path: str) -> dict:
        """
        ä¸ºå•å¼ å›¾ç‰‡ç”Ÿæˆå°çº¢ä¹¦é£æ ¼æè¿°
        """
        start_time = time.time()
        logger.info(f"å¼€å§‹å¤„ç†å›¾ç‰‡: {image_path}")
        
        try:
            # è¯»å–å¹¶ç¼–ç å›¾ç‰‡
            with open(image_path, "rb") as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
            
            # è·å–æ­£ç¡®çš„MIMEç±»å‹
            mime_type = self._get_image_mime_type(image_path)
            
            # æ„å»ºæ¶ˆæ¯ - ä¸¥æ ¼éµå¾ªå®˜æ–¹ç¤ºä¾‹æ ¼å¼
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": self.system_prompt}]
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                # ä¿®æ­£ï¼šæ·»åŠ "data:"å‰ç¼€ï¼Œè¿™æ˜¯å®˜æ–¹ç¤ºä¾‹è¦æ±‚çš„
                                "url": f"data:{mime_type};base64,{encoded_string}"
                            }
                        },
                        {
                            "type": "text",
                            "text": "è¯·ä¸ºè¿™å¼ å›¾ç‰‡åˆ›ä½œä¸€ç¯‡å°çº¢ä¹¦é£æ ¼çš„ç¬”è®°ï¼Œè¦æ±‚ç¬¦åˆç³»ç»Ÿæç¤ºä¸­çš„æ‰€æœ‰è¦ç‚¹ã€‚"
                        }
                    ]
                }
            ]
            
            logger.debug(f"è°ƒç”¨API: model={self.model_name}, image_path={image_path}")
            
            # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨APIï¼ˆä¸¥æ ¼éµå¾ªå®˜æ–¹ç¤ºä¾‹ï¼‰
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=0.7,
                max_tokens=1024
            )
            
            description = response.choices[0].message.content
            processing_time = time.time() - start_time
            logger.info(f"æˆåŠŸç”Ÿæˆæè¿° (è€—æ—¶: {processing_time:.2f}ç§’): {image_path}")
            
            # æå–å…³é”®ä¿¡æ¯
            return {
                "image_path": image_path,
                "raw_description": description,
                "title": self._extract_title(description),
                "hashtags": self._extract_hashtags(description),
                "key_details": self._extract_key_details(description),
                "processing_time": processing_time,
                "timestamp": time.time()
            }
        except Exception as e:
            error_msg = f"ç”Ÿæˆæè¿°å¤±è´¥ {image_path}: {str(e)}"
            logger.error(error_msg, exc_info=True)
            print(error_msg)
            return {
                "image_path": image_path,
                "error": str(e),
                "error_type": type(e).__name__,
                "timestamp": time.time()
            }
    
    def _extract_title(self, description: str) -> str:
        """ä»æè¿°ä¸­æå–æ ‡é¢˜"""
        if "ã€" in description and "ã€‘" in description:
            return description.split("ã€")[1].split("ã€‘")[0]
        return "å°çº¢ä¹¦é£æ ¼å›¾ç‰‡æè¿°"
    
    def _extract_hashtags(self, description: str) -> list:
        """æå–è¯é¢˜æ ‡ç­¾"""
        hashtags = []
        for word in description.split():
            if word.startswith("#"):
                hashtags.append(word[1:])
        return hashtags[:5]
    
    def _extract_key_details(self, description: str) -> list:
        """æå–å…³é”®ç»†èŠ‚"""
        details = []
        import re
        pattern = r'[â¶â·â¸â¹âº][^\n]+'
        matches = re.findall(pattern, description)
        for match in matches:
            detail = match[1:].strip()
            details.append(detail)
        return details[:5]
    
    async def process_directory(self, directory: str, batch_size: int = 2):
        """
        å¤„ç†æ•´ä¸ªç›®å½•çš„å›¾ç‰‡ï¼Œåªå¤„ç†æœªå¤„ç†è¿‡çš„å›¾ç‰‡
        """
        # è·å–ç°æœ‰ç»“æœå’Œå·²å¤„ç†çš„å›¾ç‰‡
        existing_results = self.load_existing_results()
        processed_images = self.get_processed_images(existing_results)
        
        image_paths = glob.glob(os.path.join(directory, "*.jpg")) + \
                     glob.glob(os.path.join(directory, "*.jpeg")) + \
                     glob.glob(os.path.join(directory, "*.png"))
        
        # è¿‡æ»¤æ‰å·²å¤„ç†çš„å›¾ç‰‡
        new_image_paths = [img for img in image_paths if img not in processed_images]
        
        if not new_image_paths:
            logger.info(f"ç›®å½• {directory} ä¸­æ‰€æœ‰å›¾ç‰‡å·²å¤„ç†å®Œæ¯•")
            print(f"ç›®å½• {directory} ä¸­æ‰€æœ‰å›¾ç‰‡å·²å¤„ç†å®Œæ¯•")
            # è¿”å›ç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºæ²¡æœ‰æ–°å›¾ç‰‡éœ€è¦å¤„ç†
            return []
        
        logger.info(f"åœ¨ç›®å½• {directory} ä¸­å‘ç° {len(image_paths)} å¼ å›¾ç‰‡ï¼Œ{len(new_image_paths)} å¼ æ–°å›¾ç‰‡éœ€è¦å¤„ç†")
        print(f"å‘ç° {len(new_image_paths)} å¼ æ–°å›¾ç‰‡éœ€è¦å¤„ç†...")
        
        descriptions = []
        for i in tqdm(range(0, len(new_image_paths), batch_size), desc="å¤„ç†æ–°å›¾ç‰‡"):
            batch = new_image_paths[i:i+batch_size]
            tasks = [self.generate_description(img_path) for img_path in batch]
            results = await asyncio.gather(*tasks)
            descriptions.extend(results)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            self._save_results(descriptions, f"{self.output_dir}/intermediate_results.json")
            
            # æ·»åŠ è¯·æ±‚é—´éš”
            await asyncio.sleep(2.0)
        
        return descriptions
    
    def _save_results(self, results: list, output_path: str):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"å·²ä¿å­˜ {len(results)} æ¡ç»“æœåˆ° {output_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {str(e)}", exc_info=True)

    def save_final_results(self, train_results: list, val_results: list):
        """ä¿å­˜æœ€ç»ˆç»“æœï¼Œåˆå¹¶æ–°æ—§ç»“æœ"""
        # åŠ è½½ç°æœ‰ç»“æœ
        existing_results = self.load_existing_results()
        
        # åˆ›å»ºå·²å¤„ç†å›¾ç‰‡çš„å­—å…¸ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
        existing_train_dict = {item['image_path']: item for item in existing_results['train'] if 'image_path' in item}
        existing_val_dict = {item['image_path']: item for item in existing_results['val'] if 'image_path' in item}
        
        # å°†æ–°ç»“æœæ·»åŠ åˆ°ç°æœ‰ç»“æœä¸­ï¼ˆæ–°ç»“æœä¼˜å…ˆï¼‰
        for item in train_results:
            if 'image_path' in item:
                existing_train_dict[item['image_path']] = item
        
        for item in val_results:
            if 'image_path' in item:
                existing_val_dict[item['image_path']] = item
        
        # è½¬æ¢å›åˆ—è¡¨
        train_results = list(existing_train_dict.values())
        val_results = list(existing_val_dict.values())
        
        # æ›´æ–°å…ƒæ•°æ®
        metadata = existing_results.get('metadata', {})
        metadata.update({
            "model": self.model_name,
            "system_prompt": self.system_prompt,
            "total_images": len(train_results) + len(val_results),
            "timestamp": time.time(),
            "api_key_used": self.api_key[:8] + "..." if self.api_key else "N/A"
        })
        
        final_results = {
            "train": train_results,
            "val": val_results,
            "metadata": metadata
        }
        
        output_path = f"{self.output_dir}/qwen_vl_descriptions.json"
        self._save_results(final_results, output_path)
        return output_path

async def main():
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs("descriptions", exist_ok=True)
    
    print("="*50)
    print("Qwen-VLå›¾ç‰‡æè¿°ç”Ÿæˆç³»ç»Ÿ - å¢é‡å¤„ç†ç‰ˆ")
    print("æ³¨æ„: æœ¬ç¨‹åºä¼šè‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„å›¾ç‰‡")
    print("="*50)
    
    # æ£€æŸ¥API Key
    API_KEY = "sk-5f28127ea6524835b3304469e54d05f5"
    print(f"ä½¿ç”¨API Key: {API_KEY[:8]}...{API_KEY[-4:]}")
    
    try:
        # åˆå§‹åŒ–ç®¡é“ - ä½¿ç”¨æ­£ç¡®çš„Qwen-VLæ¨¡å‹
        pipeline = QwenVLPipeline(
            api_key=API_KEY,
            model_name="qwen-vl-plus",  # ä½¿ç”¨qwen-vl-plusé¿å…Arrearageé”™è¯¯
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",  # ç¡®ä¿æ— ç©ºæ ¼
            output_dir="descriptions"
        )
        
        # æ£€æŸ¥ç°æœ‰ç»“æœ
        existing_results = pipeline.load_existing_results()
        processed_images = pipeline.get_processed_images(existing_results)
        total_processed = len(processed_images)
        
        print(f"\nâœ… ä½¿ç”¨æ¨¡å‹: {pipeline.model_name}")
        print(f"âœ… API Base URL: {pipeline.base_url}")
        print("âœ… å·²æ­£ç¡®è®¾ç½®base_urlï¼ˆæ— æœ«å°¾ç©ºæ ¼ï¼‰")
        print("âœ… å·²æ­£ç¡®è®¾ç½®image_urlæ ¼å¼ï¼ˆåŒ…å«data:å‰ç¼€ï¼‰")
        print(f"âœ… å·²æœ‰ {total_processed} å¼ å›¾ç‰‡çš„æè¿°å·²ç”Ÿæˆ")
        
        # å¤„ç†è®­ç»ƒé›†
        print("\n===== å¼€å§‹å¤„ç†è®­ç»ƒé›†å›¾ç‰‡ =====")
        train_results = await pipeline.process_directory("data/Train")
        
        # å¤„ç†éªŒè¯é›†
        print("\n===== å¼€å§‹å¤„ç†éªŒè¯é›†å›¾ç‰‡ =====")
        val_results = await pipeline.process_directory("data/Val")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_path = pipeline.save_final_results(train_results, val_results)
        print(f"\nâœ… æ‰€æœ‰æè¿°å·²ç”Ÿæˆå¹¶ä¿å­˜è‡³: {output_path}")
        
        # ç»Ÿè®¡ç»“æœ
        existing_results = pipeline.load_existing_results()
        total = len(existing_results["train"]) + len(existing_results["val"])
        success = len([r for r in existing_results["train"] + existing_results["val"] if 'error' not in r])
        failed = total - success
        new_success = len([r for r in train_results + val_results if 'error' not in r])
        
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"æ€»è®¡: {total} å¼ å›¾ç‰‡")
        print(f"æˆåŠŸ: {success} å¼ å›¾ç‰‡ ({success/total:.1%})")
        print(f"å¤±è´¥: {failed} å¼ å›¾ç‰‡ ({failed/total:.1%})")
        print(f"æœ¬æ¬¡æ–°å¢: {new_success} å¼ æˆåŠŸå›¾ç‰‡")
        print(f"æ—¥å¿—å·²ä¿å­˜åˆ°: descriptions/pipeline.log")
        
        if failed > 0:
            print("\nâš ï¸  å¦‚æœä»æœ‰å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
            print("1. ç™¾ç‚¼å¹³å°æ˜¯å¦å·²ä¸ºAPI KeyæˆæƒQwen-VLæ¨¡å‹æƒé™")
            print("2. å›¾ç‰‡æ˜¯å¦è¿‡å¤§ï¼ˆQwen-VLå•å›¾æœ€å¤§æ”¯æŒ16384 Tokenï¼‰")
            print("3. API Keyæ˜¯å¦æœ‰æ•ˆ")
    
    except Exception as e:
        logger.critical("ç¨‹åºåˆå§‹åŒ–å¤±è´¥", exc_info=True)
        print(f"\nâŒ ç¨‹åºåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        print("\nè¯·æ£€æŸ¥ä»¥ä¸‹é—®é¢˜:")
        print("1. æ˜¯å¦åœ¨ç™¾ç‚¼å¹³å°(https://bailian.console.aliyun.com)åˆ›å»ºäº†Qwen-VLåº”ç”¨?")
        print("2. æ˜¯å¦åœ¨'ä¸šåŠ¡ç©ºé—´ç®¡ç†'ä¸­ä¸ºAPI Keyæˆæƒäº†Qwen-VLæ¨¡å‹æƒé™?")
        print("3. API Keyæ˜¯å¦æ­£ç¡®?")
        print("4. base_urlæ˜¯å¦åŒ…å«å¤šä½™ç©ºæ ¼?")

if __name__ == "__main__":
    print("å¯åŠ¨ç¨‹åº...")
    try:
        asyncio.run(main())
    except Exception as e:
        import traceback
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {str(e)}")
        traceback.print_exc()