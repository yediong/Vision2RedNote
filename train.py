# åœ¨train.pyä¸­æ·»åŠ è®­ç»ƒæ•°æ®ä¿å­˜åŠŸèƒ½
import os
import json
import torch
import logging
import numpy as np
from tqdm import tqdm
from typing import List, Tuple, Dict, Any, Optional, Union
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

import transformers
from transformers import (
    Qwen2VLForConditionalGeneration,
    AutoTokenizer,
    AutoProcessor,
    BitsAndBytesConfig,
    Trainer,
    TrainingArguments,
    default_data_collator
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import EarlyStoppingCallback

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Emojiè¡¨æƒ…åº“ - å°çº¢ä¹¦é£æ ¼
EMOJI_POOL = [
    "âœ¨", "ğŸ’•", "ğŸ’–", "ğŸ’", "ğŸ’—", "ğŸ’“", "ğŸ’", "ğŸ’Ÿ", "ğŸ’Œ", "ğŸ’‹",
    "ğŸŒ¸", "ğŸŒº", "ğŸŒ·", "ğŸŒ¹", "ğŸŒ»", "ğŸŒ¼", "ï¿½ï¿½", "ğŸ€", "ï¿½ï¿½", "",
    "", "â­", "ğŸ’«", "ğŸŒ™", "â˜€ï¸", "", "â˜ï¸", "ï¿½ï¿½ï¸", "ğŸŒ¥ï¸", "ï¸",
    "ğŸ€", "ğŸˆ", "ğŸ‰", "ğŸŠ", "ğŸ‹", "ğŸ", "ğŸ", "ğŸ", "ğŸ", "ğŸ€",
    "ğŸ’", "ğŸ’", "ğŸ’", "ğŸ’’", "ğŸ’“", "ğŸ’”", "ğŸ’•", "ğŸ’–", "ğŸ’—", "ğŸ’˜",
    "ğŸ˜", "ğŸ¥°", "ğŸ˜˜", "ğŸ˜‹", "ğŸ˜Š", "ğŸ˜‰", "ğŸ˜Œ", "ğŸ˜‡", "ğŸ¤—", "ğŸ¤©",
    "ğŸ”¥", "ğŸ’¯", "ğŸ’ª", "ğŸ‘", "ğŸ™Œ", "ğŸ¤", "ğŸ‘", "ğŸ‘Œ", "âœŒï¸", "ğŸ¤",
    "ğŸ¯", "ğŸª", "ğŸ¨", "ğŸ­", "ğŸ¬", "ğŸ¤", "ï¿½ï¿½", "", "ğŸ¶", ""
]

# æ•°æ®é›†ç±»
class QwenVLDataset(torch.utils.data.Dataset):
    def __init__(self, json_path, processor, max_length=2048):
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        all_items = data['train'] + data['val']
        self.items = []
        
        # éªŒè¯å›¾åƒæ–‡ä»¶
        for item in all_items:
            # æ£€æŸ¥æ˜¯å¦æœ‰æè¿°å­—æ®µ
            description = None
            possible_fields = ['raw_description', 'description', 'text', 'caption', 'content']
            for field in possible_fields:
                if field in item:
                    description = item[field]
                    break
            
            # å¦‚æœæ²¡æœ‰æè¿°å­—æ®µï¼Œè·³è¿‡è¿™ä¸ªitem
            if description is None:
                logger.warning(f"Skipping item without description: {list(item.keys())}")
                continue
            
            # æ£€æŸ¥æè¿°é•¿åº¦ï¼Œå¤ªçŸ­çš„æè¿°å¯èƒ½è´¨é‡ä¸å¥½
            if len(description.strip()) < 10:
                logger.warning(f"Skipping item with too short description: {description[:50]}...")
                continue
            
            image_path = item['image_path'].replace('\\', '/')
            if not os.path.exists(image_path):
                # å°è¯•ä»dataç›®å½•ä¸‹æŸ¥æ‰¾
                image_path = os.path.join("data", os.path.basename(image_path))
            
            if os.path.exists(image_path):
                try:
                    # å°è¯•æ‰“å¼€å›¾åƒæ–‡ä»¶
                    with Image.open(image_path) as img:
                        img.verify()  # éªŒè¯å›¾åƒå®Œæ•´æ€§
                    item['image_path'] = image_path  # æ›´æ–°ä¸ºæ­£ç¡®çš„è·¯å¾„
                    item['description'] = description  # ç»Ÿä¸€å­—æ®µåä¸ºdescription
                    self.items.append(item)
                except Exception as e:
                    logger.warning(f"Invalid image file {image_path}: {str(e)}")
            else:
                logger.warning(f"Image file not found: {image_path}")
        
        self.processor = processor
        self.max_length = max_length
        logger.info(f"Loaded {len(self.items)} valid items")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if len(self.items) > 0:
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªitemçš„å­—æ®µ
            first_item = self.items[0]
            logger.info(f"First item fields: {list(first_item.keys())}")
            logger.info(f"First item description: {first_item.get('description', 'NO DESCRIPTION')[:100]}...")

    def __len__(self):
        return len(self.items)

    # åœ¨train.pyä¸­ä¿®æ”¹QwenVLDatasetç±»çš„__getitem__æ–¹æ³•
    def __getitem__(self, idx):
        item = self.items[idx]
        image_path = item['image_path']
        description = item['description']  # ä½¿ç”¨ç»Ÿä¸€çš„å­—æ®µå
        
        # è¯»å–å›¾åƒ
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            logger.error(f"Error loading image {image_path}: {str(e)}")
            return {}
        
        # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´è‡ªç„¶çš„å°çº¢ä¹¦é£æ ¼promptï¼Œé¿å…åˆ†æ®µæ ¼å¼
        selected_prompt = "è¯·ç”¨å°çº¢ä¹¦åšä¸»çš„è¯­æ°”æè¿°è¿™å¼ å›¾ç‰‡ï¼Œè¦æ´»æ³¼å¯çˆ±ï¼Œå¤šç”¨emojiè¡¨æƒ…ï¼âœ¨"
        
        # ä¼˜åŒ–ï¼šå¯¹åŸå§‹æè¿°è¿›è¡Œå°çº¢ä¹¦é£æ ¼å¢å¼ºï¼Œæé«˜emojiå¯†åº¦
        enhanced_description = self._enhance_xiaohongshu_style(description)
        
        # æ„å»ºå¯¹è¯æ ¼å¼ - ä½¿ç”¨æ›´è‡ªç„¶çš„prompt
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": selected_prompt}
                ],
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": enhanced_description}
                ]
            }
        ]
        
        # æ­£ç¡®è®¡ç®—ç”¨æˆ·æç¤ºé•¿åº¦çš„æ–¹æ³•
        full_prompt = self.processor.apply_chat_template(
            conversation, 
            tokenize=False, 
            add_generation_prompt=False
        )
        
        # æ­£ç¡®æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨tokenizerè®¡ç®—é•¿åº¦
        full_tokens = self.processor.tokenizer(
            full_prompt,
            return_tensors="pt",
            add_special_tokens=True
        )
        
        # æ‰¾åˆ°assistantå›å¤å¼€å§‹çš„ä½ç½®
        assistant_start_token = self.processor.tokenizer.encode("<|im_start|>assistant")[0]
        input_ids = full_tokens.input_ids[0]
        
        # åˆ›å»ºæ­£ç¡®çš„labels
        labels = input_ids.clone()
        
        # æ‰¾åˆ°assistantå¼€å§‹çš„ä½ç½®
        try:
            assistant_pos = (input_ids == assistant_start_token).nonzero(as_tuple=True)[0][0].item() + 1
            # å°†assistantä¹‹å‰çš„å†…å®¹è®¾ä¸º-100
            labels[:assistant_pos] = -100
        except:
            # å¦‚æœæ‰¾ä¸åˆ°assistant tokenï¼Œä¿å®ˆå¤„ç†ï¼šåªè®¡ç®—æœ€å1/3çš„æŸå¤±
            labels[:len(labels)//3*2] = -100
        
        return {
            'input_ids': input_ids,
            'attention_mask': full_tokens.attention_mask[0],
            'labels': labels
        }

    def _enhance_xiaohongshu_style(self, description):
        """å¢å¼ºå°çº¢ä¹¦é£æ ¼ï¼Œæé«˜emojiå¯†åº¦"""
        import random
        
        # æ‰©å±•emojiå‰ç¼€å’Œåç¼€åº“
        emoji_prefixes = ["âœ¨", "ğŸ’•", "ğŸ’–", "ğŸ’", "ğŸŒ¸", "ğŸŒŸ", "ğŸ’«", "ğŸ€", "ğŸ’", "ï¿½ï¿½"]
        emoji_suffixes = ["âœ¨", "ğŸ’•", "ğŸ’–", "ğŸ”¥", "ğŸ’¯", "ğŸ’ª", "ğŸ‘", "ğŸ‰", "ğŸ’", "ï¿½ï¿½"]
        emoji_middle = ["ğŸ’•", "ğŸ’–", "ğŸ’“", "ğŸ’—", "ğŸ’", "ğŸŒ¸", "ğŸŒº", "ğŸŒ·", "ğŸŒ¹", "ğŸŒ»", "ğŸŒ¼", "ï¿½ï¿½", "ï¿½ï¿½", "ï¿½ï¿½", "â­", "ğŸ’«", "ğŸŒ™", "â˜€ï¸", "ğŸ€", "ğŸˆ", "ğŸ‰", "ğŸŠ", "ğŸ’", "ğŸ’", "ğŸ’", "ğŸ˜", "ğŸ¥°", "ğŸ˜˜", "ğŸ˜‹", "ğŸ˜Š", "ğŸ˜‰", "ğŸ˜Œ", "ğŸ˜‡", "ğŸ¤—", "ğŸ¤©", "ğŸ”¥", "ğŸ’¯", "ğŸ’ª", "ğŸ‘", "ğŸ™Œ", "ğŸ¤", "ğŸ‘", "ğŸ‘Œ", "âœŒï¸", "ï¿½ï¿½"]
        
        # å°çº¢ä¹¦é£æ ¼è¯æ±‡æ›¿æ¢
        style_replacements = {
            "å¥½çœ‹": "ç»ç¾",
            "æ¼‚äº®": "ç¥ä»™é¢œå€¼",
            "å–œæ¬¢": "è¶…çˆ±",
            "æ¨è": "å®‰åˆ©",
            "ä¸é”™": "å¤ªå¯äº†",
            "å¥½": "ç»äº†",
            "æ£’": "å®è—",
            "ç¾": "ç»ç¾",
            "å¯çˆ±": "è¶…å¯çˆ±",
            "å–œæ¬¢": "è¶…çˆ±",
            "æ¨è": "å®‰åˆ©",
            "åˆ†äº«": "ç§è‰",
            "å€¼å¾—": "è¶…å€¼",
            "æ€§ä»·æ¯”": "æ€§ä»·æ¯”è¶…é«˜",
            "å…¥æ‰‹": "æ‹”è‰",
            "è´­ä¹°": "å…¥æ‰‹",
            "ä½¿ç”¨": "ä½“éªŒ",
            "æ„Ÿå—": "ä½“éªŒæ„Ÿ",
            "æ•ˆæœ": "æ•ˆæœç»äº†",
            "è´¨é‡": "å“è´¨è¶…æ£’"
        }
        
        # é¿å…åºå·æ ¼å¼çš„è¯æ±‡æ›¿æ¢
        avoid_numbering = {
            "ç¬¬ä¸€": "é¦–å…ˆ",
            "ç¬¬äºŒ": "å…¶æ¬¡", 
            "ç¬¬ä¸‰": "æœ€å",
            "1.": "",
            "2.": "",
            "3.": "",
            "ä¸€ã€": "",
            "äºŒã€": "",
            "ä¸‰ã€": "",
            "1ã€": "",
            "2ã€": "",
            "3ã€": ""
        }
        
        # è¯æ±‡æ›¿æ¢
        for old_word, new_word in style_replacements.items():
            if old_word in description:
                description = description.replace(old_word, new_word)
        
        # é¿å…åºå·æ ¼å¼
        for old_word, new_word in avoid_numbering.items():
            if old_word in description:
                description = description.replace(old_word, new_word)
        
        # éšæœºæ·»åŠ emojiå‰ç¼€ (80%æ¦‚ç‡)
        if random.random() < 0.8:
            description = random.choice(emoji_prefixes) + " " + description
        
        # éšæœºåœ¨å¥å­ä¸­é—´æ’å…¥emoji (60%æ¦‚ç‡)
        if random.random() < 0.6 and len(description) > 20:
            # åœ¨å¥å­ä¸­é—´éšæœºä½ç½®æ’å…¥emoji
            words = description.split()
            if len(words) > 3:
                insert_pos = random.randint(1, len(words) - 1)
                words.insert(insert_pos, random.choice(emoji_middle))
                description = " ".join(words)
        
        # éšæœºæ·»åŠ emojiåç¼€ (80%æ¦‚ç‡)
        if random.random() < 0.8:
            description = description + " " + random.choice(emoji_suffixes)
        
        # éšæœºåœ¨å¥å­æœ«å°¾æ·»åŠ æ„Ÿå¹å· (70%æ¦‚ç‡)
        if random.random() < 0.7 and not description.endswith(('ï¼', '!')):
            description = description + "ï¼"
        
        # éšæœºåœ¨å¥å­ä¸­é—´æ·»åŠ æ„Ÿå¹å· (40%æ¦‚ç‡)
        if random.random() < 0.4 and len(description) > 15:
            # åœ¨å¥å­ä¸­é—´éšæœºä½ç½®æ·»åŠ æ„Ÿå¹å·
            if 'ï¼Œ' in description:
                parts = description.split('ï¼Œ')
                if len(parts) > 1:
                    insert_pos = random.randint(0, len(parts) - 1)
                    parts[insert_pos] = parts[insert_pos] + "ï¼"
                    description = 'ï¼Œ'.join(parts)
        
        return description

# è‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨
def collate_fn(batch):
    # è¿‡æ»¤æ‰æ— æ•ˆæ ·æœ¬
    batch = [item for item in batch if item is not None and len(item.get('input_ids', [])) > 0]
    
    if not batch:
        # è¿”å›ä¸€ä¸ªç©ºçš„batchè€Œä¸æ˜¯None
        return {
            'input_ids': torch.empty(0, 1, dtype=torch.long),
            'attention_mask': torch.empty(0, 1, dtype=torch.long),
            'labels': torch.empty(0, 1, dtype=torch.long)
        }
    
    # æ‰¾åˆ°æœ€å¤§é•¿åº¦
    max_len = max(len(item['input_ids']) for item in batch)
    
    # å¡«å……åˆ°æœ€å¤§é•¿åº¦
    input_ids = []
    attention_mask = []
    labels = []
    
    for item in batch:
        seq_len = len(item['input_ids'])
        padding_len = max_len - seq_len
        
        input_ids.append(torch.cat([
            item['input_ids'],
            torch.full((padding_len,), 0, dtype=torch.long)  # ä½¿ç”¨0ä½œä¸ºpad token
        ]))
        
        attention_mask.append(torch.cat([
            item['attention_mask'],
            torch.zeros(padding_len, dtype=torch.long)
        ]))
        
        labels.append(torch.cat([
            item['labels'],
            torch.full((padding_len,), -100, dtype=torch.long)
        ]))
    
    return {
        'input_ids': torch.stack(input_ids),
        'attention_mask': torch.stack(attention_mask),
        'labels': torch.stack(labels)
    }

# è‡ªå®šä¹‰Trainerç±»ï¼Œç”¨äºè®°å½•è®­ç»ƒæ•°æ®
class CustomTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.train_losses = []
        self.eval_losses = []
        self.learning_rates = []
        
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # æ·»åŠ **kwargsæ¥æ¥æ”¶é¢å¤–å‚æ•°
        result = super().compute_loss(model, inputs, return_outputs)
        
        # å¤„ç†è¿”å›å€¼ï¼Œå¯èƒ½æ˜¯å•ä¸ªlossæˆ–(loss, outputs)å…ƒç»„
        if isinstance(result, tuple):
            loss = result[0]
        else:
            loss = result
        
        # è®°å½•è®­ç»ƒæŸå¤±
        if self.state.global_step % 10 == 0:  # æ¯10æ­¥è®°å½•ä¸€æ¬¡
            self.train_losses.append(loss.item())
            # å®‰å…¨åœ°è·å–å­¦ä¹ ç‡
            try:
                if hasattr(self, 'lr_scheduler') and self.lr_scheduler is not None:
                    self.learning_rates.append(self.lr_scheduler.get_last_lr()[0])
                else:
                    self.learning_rates.append(0.0)
            except:
                self.learning_rates.append(0.0)
        
        return result
    
    def evaluation_step(self, model, inputs):
        loss = super().evaluation_step(model, inputs)
        self.eval_losses.append(loss.loss.item())
        return loss
    
    def save_training_data(self, output_dir):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        training_data = {
            'train_losses': self.train_losses,
            'eval_losses': self.eval_losses,
            'learning_rates': self.learning_rates,
            'total_steps': len(self.train_losses) * 10,
            'eval_steps': len(self.eval_losses)
        }
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        training_data_path = os.path.join(output_dir, 'training_data.json')
        with open(training_data_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {training_data_path}")
        
# ä¸»è®­ç»ƒå‡½æ•°
def train():
    # æ¨¡å‹å’Œè®­ç»ƒå‚æ•°
    model_name = "./Qwen2-VL-2B-Instruct"
    json_path = "data/qwen_vl_descriptions.json"
    output_dir = "./qwen2-finetuned0824_3"  # æ–°çš„è¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # 4-bité‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    
    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    logger.info("Loading model and processor...")
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    # å‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ
    model = prepare_model_for_kbit_training(model)
    
    # ä¼˜åŒ–LoRAé…ç½®
    lora_config = LoraConfig(
        r=32,  # å¢åŠ rankï¼Œæé«˜è¡¨è¾¾èƒ½åŠ›
        lora_alpha=64,  # å¢åŠ alpha
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                        "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.1,  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # åŠ è½½tokenizerå’Œprocessor
    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info("Loading dataset...")
    dataset = QwenVLDataset(json_path, processor)
    
    # æ£€æŸ¥æ•°æ®é›†å¤§å°
    if len(dataset) == 0:
        raise ValueError("æ•°æ®é›†ä¸ºç©ºï¼è¯·æ£€æŸ¥JSONæ–‡ä»¶å’Œå›¾åƒè·¯å¾„ã€‚")
    
    # ä¼˜åŒ–ï¼šå¢åŠ è®­ç»ƒæ•°æ®é‡
    train_size = min(2000, len(dataset)) 
    train_dataset = torch.utils.data.Subset(dataset, range(train_size))
    eval_dataset = torch.utils.data.Subset(dataset, range(train_size, len(dataset)))
    
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(eval_dataset)}")
    
    # ä¼˜åŒ–è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=6,  # å¢åŠ è®­ç»ƒè½®æ•°
        per_device_train_batch_size=1,
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=16,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
        learning_rate=1e-3,  # é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®š
        weight_decay=0.01,
        logging_dir=f"{output_dir}/logs",
        logging_steps=10,
        save_strategy="epoch",
        eval_strategy="epoch",
        fp16=True,
        report_to="tensorboard",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine_with_restarts",  # ä½¿ç”¨å¸¦é‡å¯çš„cosineè°ƒåº¦
        warmup_ratio=0.1,  # å¢åŠ warmupæ¯”ä¾‹
        remove_unused_columns=False,
        gradient_checkpointing=True,
        dataloader_pin_memory=False,
        max_grad_norm=0.5,  # å¢åŠ æ¢¯åº¦è£å‰ªé˜ˆå€¼
        warmup_steps=200,  # å¢åŠ warmupæ­¥æ•°
        save_total_limit=2,  # åªä¿å­˜æœ€å¥½çš„2ä¸ªæ¨¡å‹
    )
    
    # åˆ›å»ºè‡ªå®šä¹‰Trainer
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=collate_fn,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],  # å¢åŠ patience
    )
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("Starting training...")
    train_result = trainer.train()
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    trainer.save_training_data(output_dir)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("Saving final model...")
    model.save_pretrained(output_dir)
    processor.save_pretrained(output_dir)
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    logger.info("Training completed successfully!")

if __name__ == "__main__":
    train()